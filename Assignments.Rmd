---
title: "Assignments for Crim 250"
output:
  html_document:
    toc: yes
    toc_float: yes
    collapsed: no
    number_sections: no
    toc_depth: 1
  pdf_document:
    toc: no
    toc_depth: '1'
---


This page will contain all the assignments you submit for the class.

This page will contain all the assignments you submit for the class.



### Instructions for all assignments

I want you to submit your assignment as a PDF, so I can keep a record of what the code looked like that day. I also want you to include your answers on your personal GitHub website. This will be good practice for editing your website and it will help you produce something you can keep after the class is over.

1. Download the Assignment1.Rmd file from Canvas. You can use this as a template for writing your answers. It's the same as what you can see on my website in the Assignments tab. Once we're done with this I'll edit the text on the website to include the solutions.

2. On RStudio, open a new R script in RStudio (File > New File > R Script). This is where you can test out your R code. You'll write your R commands and draw plots here.

3. Once you have finalized your code, copy and paste your results into this template (Assignment 1.Rmd). For example, if you produced a plot as the solution to one of the problems, you can copy and paste the R code in R markdown by using the ` ``{r} ``` ` command. Answer the questions in full sentences and Save.

4. Produce a PDF file with your answers. To do this, knit to PDF (use Knit button at the top of RStudio), locate the PDF file in your docs folder (it's in the same folder as the Rproj), and submit that on on Canvas in Assignment 1.

5. Build Website, go to GitHub desktop, commit and push. Now your solutions should be on your website as well.






# Assignment 1

**Collaborators: none. **

This assignment is due on Canvas on Monday 9/20 before class, at 10:15 am. Include the name of anyone with whom you collaborated at the top of the assignment.


### Problem 1 

Install the datasets package on the console below using `install.packages("datasets")`. Now load the library.

```{r}
# install.packages("datasets")
library(datasets)
```

Load the USArrests dataset and rename it `dat`. Note that this dataset comes with R, in the package datasets, so there's no need to load data from your computer. Why is it useful to rename the dataset?

```{r}
USArrests
dat <- USArrests
```
Answer: It is useful to rename the data set because it makes it easier to remember the name we give it to use in code commands later on in the assignment. It helps to separate the base R package dataset and turn it into our own dataset to use with the following work. 

### Problem 2

Use this command to make the state names into a new variable called State. 

```{r}
dat$state <- tolower(rownames(USArrests))
```

This dataset has the state names as row names, so we just want to make them into a new variable. We also make them all lower case, because that will help us draw a map later - the map function requires the states to be lower case.


List the variables contained in the dataset `USArrests`.

```{r}
list(dat)
head(dat)
names(dat)
summary(dat)

```

### Problem 3 

What type of variable (from the DVB chapter) is `Murder`? 

Answer: Murder is a quantitative variable, because there are measured numerical values representing murder rates for each state. 

What R Type of variable is it?

Answer: Murder is a character variable, as seen in the summary (dat) description in the code above.


### Problem 4

What information is contained in this dataset, in general? What do the numbers mean? 

Answer: This dataset contains the number of Murders, Assaults, and Rapes in each of the 50 states in the year 1973. It also shows us how many people lived in an urban area each state in that year, demonstrated by "UrbanPop."

### Problem 5

Draw a histogram of `Murder` with proper labels and title.

```{r, echo = TRUE, fig.width = 5, fig.height = 8}
# histogram of Murder
hist(dat$Murder, main="Histogram of Murder", xlab="Murder Rate", ylab="Frequency")

```

### Problem 6

Please summarize `Murder` quantitatively. What are its mean and median? What is the difference between mean and median? What is a quartile, and why do you think R gives you the 1st Qu. and 3rd Qu.?

```{r}
#Summary of Murder
summary(dat$Murder)
```
Answer: The mean of Murder is 7.788, the median of Murder is 7.250. Mean represents the average number of murders between the 50 states in 1973, whereas mean is the middle number of murders if the quantities were lined up in numerical order. 
Quartiles are 4 approximately evenly sized groups with the data ordered from least to greatest. R most likely gives us Q1 and Q3 because the values between these two represent the middle 50% of the data, and provide more context for the spread of the mean. 

### Problem 7

Repeat the same steps you followed for `Murder`, for the variables `Assault` and `Rape`. Now plot all three histograms together. You can do this by using the command `par(mfrow=c(3,1))` and then plotting each of the three. 

```{r, echo = TRUE, fig.width = 5, fig.height = 8}
#histogram of Assault
hist(dat$Assault, main="Histogram of Assault", xlab="Assault Rate", ylab="Frequency")
#Summary of Assault
summary(dat$Assault)

#Histogram of Rape
hist(dat$Rape, main="Histogram of Rape", xlab="Rape Rate", ylab="Frequency")
#Summary of Rape
summary(dat$Rape)

#Histogram of Murder, Assault, Rape
par(mfrow=c(3,1))
hist(dat$Murder, main="Histogram of Murder", xlab="Murder Rate", ylab="Frequency")
hist(dat$Assault, main="Histogram of Assault", xlab="Assault Rate", ylab="Frequency")
hist(dat$Rape, main="Histogram of Rape", xlab="Rape Rate", ylab="Frequency")
```

What does the command par do, in your own words (you can look this up by asking R `?par`)?

Answer: the command par is used to set certain parameters within the data given to it. In this case, we used the par function to tell R that we wanted to plot 3 character vectors on histograms together in one plot.

What can you learn from plotting the histograms together?

Answer: Plotting the 3 histograms together allows us to easily visually compare the frequency of murders, assaults, and rapes to see which crime was most common among the 50 states in the year 1973. 
  
### Problem 8

In the console below (not in text), type `install.packages("maps")` and press Enter, and then type `install.packages("ggplot2")` and press Enter. This will install the packages so you can load the libraries.

Run this code:

```{r, fig.width = 7.5, fig.height = 4}
library('maps') 
library('ggplot2') 

ggplot(dat, aes(map_id=state, fill=Murder)) + 
  geom_map(map=map_data("state")) + 
  expand_limits(x=map_data("state")$long, y=map_data("state")$lat)
```

What does this code do? Explain what each line is doing.

Answer: These lines of code are generating a dataframe that shows the map of the US,  breaking up the information from the variable Murder into sections based on state. The lines also color-code the map based on the number of murders that occurred in that state. 

$$\\[2in]$$

# Assignment 2
## Subtitle: Crim 250: Statistics for the Social Sciences
Name: Johanna Doherty
Date: 09/24/2021


## Problem 1: Load data

Set your working directory to the folder where you downloaded the data.
setwd("/Users/johannadoherty/Documents/FALL 21/CRIM 250/Assign 2")

Read the data
dat <- read.csv(file = 'dat.nsduh.small.1.csv')

What are the dimensions of the dataset? 
names(dat)
dim(dat)

Answer: The dimensions of the data are 7 variables/columns by 171 rows as seen above in the dimensions. 

## Problem 2: Variables

 Describe the variables in the dataset.
Answer: These are all numeric variables in R, although not all are quantitative variables in the data set. For example, irsex is a qualitative variable, with two options male or female, but each sex has been coded to a corresponding number, either 1 or 2, to make it useful as a numeric variable in R. 

What is this dataset about? Who collected the data, what kind of sample is it, and what was the purpose of generating the data?
Answer: The dataset is about drug use and health in the United States. The NSDUH (National Survey of Drug Use and Health) collected the data from participants ages 12 years and older in 2019. This is an example of a state based sample survey. The purpose of collecting the data from this sample is to hopefully be able to generalize about the drug use habits and health of the wider US population. 


## Problem 3: Age and gender

What is the age distribution of the sample like? Make sure you read the codebook to know what the variable values mean.
hist(dat$age2)
Answer: I used a histogram of the age2 variable (shown below) to demonstrate the distribution of age. Using the codebook to look at the meaning of the age2 variable, I can tell that the majority of respondents were between 35 and 49 years old (represented by the number 15 on this histogram). There was not a normal distribution of ages in this dataset, in fact the ages are skewed to the right, meaning there are more older participants than younger. 

Do you think this age distribution representative of the US population? Why or why  not?
I do not think this is entirely representative of the distribution of ages in the US Population. While I do believe there are a lot of Americans falling into the 35-49 age group, I do not think there is such a drastically higher frequency of people of this age group than others. It is likely that this age group was just the most likely to fill out this kind of survey. 

Is the sample balanced in terms of gender? If not, are there more females or males?
hist(dat$irsex)
summary(dat$irsex)

Answer: (in this above histogram, 1 represents male and 2 represents female). The dataset is almost balanced in terms of gender, as can be seen in the histogram. However, there are slightly more males in the data set than females. This can also be seen when looking at summary data of the gender variable (screenshot above), the mean is 1.468, meaning there are slightly more males than females in this data set. If it were perfectly balanced the mean should be exactly 1.5. 


Use this code to draw a stacked bar plot to view the relationship between sex and age. What can you conclude from this plot?
tab.agesex <- table(dat$irsex, dat$age2)
barplot(tab.agesex,
        main = "Stacked barchart",
        xlab = "Age category", ylab = "Frequency",
        legend.text = rownames(tab.agesex),
        beside = FALSE) # Stacked bars (default)

Answer: From this plot we can conclude that while most age group are roughly evenly split between males and females, some categories are overwhelmingly one or the other. For example, age group 8 (19 years old) seems to be entirely female, while age groups 6 and 7 (17 and 18 years old) seem to be entirely male. Overall, there are roughly even amounts of males and females for each age group, but there are some outliers. 

## Problem 4: Substance use
For which of the three substances included in the dataset (marijuana, alcohol, and cigarettes) do individuals tend to use the substance earlier?
hist(dat$mjage)
hist(dat$cigage)
hist(dat$iralcage)

Answer: From these histograms, it appears that a higher frequency of respondents first tried marijuana between the ages of 10-15 than any other substance. For both alcohol and cigarettes, the tallest/highest frequency age blocks were older/further to the right than for marijuana.

## Problem 5: Sexual attraction

What does the distribution of sexual attraction look like? Is this what you expected?
table(dat$sexatract)

(For this variable; 1=heterosexual, 2=mostly attracted to opposite sex, 3=equally attracted to males and females, 4=mostly attracted to same sex, 5=only attracted to same sex, 6=not sure, 97=refused, 98=blank, 99=legitimate skip.)

Answer: It looks like the vast majority of respondents are heterosexual (only attracted to the opposite gender), which is pretty much expected given the demographics of the US population. 

 What is the distribution of sexual attraction by gender? 
tab.gendersexatract <- table(dat$sexatract, dat$irsex)
barplot(tab.gendersexatract,
        main = "Stacked barchart",
        xlab = "Gender Category", ylab = "Frequency",
        legend.text = rownames(tab.gendersexatract),
        beside = FALSE) # Stacked bars (default)
        
Answer: The distribution of sexual attraction by gender show that the sexual orientations of the females in this data set were more varied than those of the males. While the majority of both sexes still identified as straight, there were a higher frequency of women who identified as something other than straight as compared to the males. 


## Problem 6: English speaking

What does the distribution of English speaking look like in the sample? Is this what you might expect for a random sample of the US population?
table(dat$speakengl)

Answer: The vast majority of respondents answered that they speak English very well, with only 21 selecting Well, and very few selecting anything less proficient than that. This does make sense to me, given that the national language of the US is English, but there are also a significant number of immigrants in the US that may not be as proficient in English. I would say that this is close to what I expected, but I may have expected to see more less proficient answers. 

Are there more English speaker females or males?
tab.sexspeakengl <- table(dat$speakengl, dat$irsex)
barplot(tab.sexspeakengl,
        main = "Stacked barchart",
        xlab = "Sex", ylab = "Frequency",
        legend.text = rownames(tab.sexspeakengl),
        beside = FALSE) # Stacked bars (default)

Answer: There are more English speaking males than females, but this is mainly because there are more males in the data set overall. There looks to be higher percentage of women that speak English “very well” compared to the men, but because there are a higher number of men overall, there are a higher number of men that speak English very well overall as well. 






# "Exam 1"
## author: "Johanna Doherty"
date: "10/04/2021"
output: html_document

Instructions

a. Create a folder in your computer (a good place would be under Crim 250, Exams). 

b. Download the dataset from the Canvas website (fatal-police-shootings-data.csv) onto that folder, and save your Exam 1.Rmd file in the same folder.

c. Download the README.md file. This is the codebook. 

d. Load the data into an R data frame.
```{r}
setwd("/Users/johannadoherty/Documents/FALL 21/CRIM 250/EXAM 1")
dat<-read.csv("fatal-police-shootings-data.csv")
```


Problem 1 (10 points)

a. Describe the dataset. This is the source: https://github.com/washingtonpost/data-police-shootings . Write two sentences (max.) about this.

__This data set, run by the Washington Post, contains every fatal shooting by U.S. police officers in the line of duty since January 2015.The data includes name of the victim, the date of the shooting, the manner of death, race, city, state and other relevant information about each shooting.__

b. How many observations are there in the data frame?
```{r}
dim(dat)
```

__There are 6594 observations for each of 17 variables in this data set..__

c. Look at the names of the variables in the data frame. Describe what "body_camera", "flee", and "armed" represent, according to the codebook. Again, only write one sentence (max) per variable.
```{r}
names(dat)
```

__Body_Camera indicates whether or not the officer was wearing a body camera at the time of the shooting. Flee represents whether the victim was moving away from officers and whether they were moving by foot or by car or other. Armed represents whether the victim was armed at the time of the shooting, and if so describes what they were armed with. __

d. What are three weapons that you are surprised to find in the "armed" variable? Make a table of the values in "armed" to see the options.
```{r}
table(dat$armed)
```

__I was definitely surprised to see an air conditioner, a nail gun, and wasp spray used as weapons against the police. __

Problem 2 (10 points)

a. Describe the age distribution of the sample. Is this what you would expect to see?
```{r}
hist(dat$age)
```

__The distribution of age is skewed to the right, with the center focused around 30 years of age. This is pretty much what I would expect the age distribution to be, the most common age for crime to be committed is usually around 30 or under, so this may be a group that the police is most wary or suspicious of.__

b. To understand the center of the age distribution, would you use a mean or a median, and why? Find the one you picked.
```{r}
summary(dat$age)
```

__Since the data are slightly skewed, I will use the median to understand the center of the distribution, because this will reduce the impact of outliers on the center. The median of these data is 35 years of age.__

c. Describe the gender distribution of the sample. Do you find this surprising?
```{r}
table(dat$gender)
```

__There are way more male victims of fatal police shootings than females, as seen in the table above. This is not at all surprising to me, in America the stereotypical "criminal" is usually thought of as a young man, so it makes sense that the police might feel most threatened by people that fall into this category.__

 Problem 3 (10 points)

a. How many police officers had a body camera, according to news reports? What proportion is this of all the incidents in the data? Are you surprised that it is so high or low?

```{r}
table(dat$body_camera)
```

__910 of the recorded police officers were wearing a body camera, which is approximately 14% of the total recorded incidents. I am not surprised by how low this percentage is, I think that police officers have been generally resistant to the use of body cameras while in the field, and there are still many jurisdictions which do not enforce their use.__

b. In  how many of the incidents was the victim fleeing? What proportion is this of the total number of incidents in the data? Is this what you would expect?
```{r}
table(dat$flee)
counts <- table(dat$flee)
barplot(counts)
```

__It is important to note that there is one unlabeled category in the fleeing variable, that represents 491 incidents. Given that we can not tell what this represents, we will omit it from the number and percentage of those who fled/did not flee. 2151 of the victims were fleeing at the time of the shooting. This represents roughly 35% of the total incidents. I would actually expect more than this percentage of victims to have been fleeing the scene, I'm surprised that a majority of the victims recorded were not fleeing than those who were.__



Problem 4 (10 points) -  Answer only one of these (a or b).

a. Describe the relationship between the variables "body camera" and "flee" using a stacked barplot. What can you conclude from this relationship? 

*Hint 1: The categories along the x-axis are the options for "flee", each bar contains information about whether the police officer had a body camera (vertically), and the height along the y-axis shows the frequency of that category).*

*Hint 2: Also, if you are unsure about the syntax for barplot, run ?barplot in R and see some examples at the bottom of the documentation. This is usually a good way to look up the syntax of R code. You can also Google it.*


```{r}
tab.bodycamflee<-table(dat$body_camera, dat$flee)
barplot(tab.bodycamflee, main = "Stacked Barchart", xlab = "Fleeing", ylab = "Body Camera", legend.text = rownames(tab.bodycamflee),beside = FALSE) # Stacked bars (default)

```

__It is important to note again that there is one unlabeled category in the fleeing variable, that represents 491 incidents. Given that we can not tell what this represents, we will omit it from the analysis of the barchart. In the above stacked barchat, we can see that there seems to be a similar proportion of officers wearing body cameras for each category of fleeing/not fleeing. Although we can see that the Not fleeing category has a higher number of officers wearing body cameras, there are also a higher number of not fleeing incidents as compared to the other categories, so proportionally they look pretty similar across the board. From this distribution, it seems that there is no obvious relationship between body camera wearing and victim fleeing patterns, as there are very similar proportions of body cameras for each category of fleeing. It's possible that a stacked barchart was not the best option to represent the relationship between these data, as we can not surmise much about the relationship from looking at this graph. __

b. Describe the relationship between age and race by using a boxplot. What can you conclude from this relationship? 

*Hint 1: The categories along the x-axis are the race categories and the height along the y-axis is age.* 

*Hint 2: Also, if you are unsure about the syntax for boxplot, run ?boxplot in R and see some examples at the bottom of the documentation. This is usually a good way to look up the syntax of R code. You can also Google it.*


```{r}

```

__Your answer here.__






Extra credit (10 points)

a. What does this code tell us? 

```{r, eval=FALSE}
mydates <- as.Date(dat$date)
head(mydates)
(mydates[length(mydates)] - mydates[1])
```
__This code shows us the dates when each of the recorded shootings took place, and by looking at the first few rows of data, we can see that there were 6 shootings in the first 4 days of 2015. The last line of code tells us the number of days between the first recorded shooting in the dataset and the most recent, which is 2458 days or 6.7 years. Given that the first dates are in January 2015 and we are now in October of 2021, this time difference makes sense.__

b. On Friday, a new report was published that was described as follows by The Guardian: "More than half of US police killings are mislabelled or not reported, study finds." Without reading this article now (due to limited time), why do you think police killings might be mislabelled or underreported?

__I think that police shootings could be mis-labelled or under-reported due to them being classified as less severe than they are. For example, if a victim is shot by the police, but does not die until they are in the hospital hours later, this might be classified as just a shooting and not necessarily a fatal shooting since the victim did not die on the scene. Although there should be no gray area, and any person who dies after being shot by the police should be recorded as a fatal police shooting, I can see the potential for mislabelling and underreporting with situations that occurr under murkier circumstances. .__

c. Regarding missing values in problem 4, do you see any? If so, do you think that's all that's missing from the data?

__In the flee variable, there is an unlabeled category. Since we can not determine what these 491 incidents represent in terms of fleeing, we have to omit them from our analysis of the fleeing variable, which means we treat them as missing values. Given that this missing category exists, it would not surprise me to see that other variables have missing data as well. Given that "fleeing" could be seen as a matter of opinion or a subjective question in some challenging circumstances, it makes sense to me that there might be missing data in this variable. It seems possible that some of the other variables with "gray areas" or that are less clear cut could have missing data as well.__





# "Assignment 3"
## "Johanna Doherty"
date: "Today's date here: 10/26/2021"
output: html_document


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Collaborators: **.

This assignment is due on Canvas on Wednesday 10/27/2021 before class, at 10:15 am. Include the name of anyone with whom you collaborated at the top of the assignment.

Submit your responses as either an HTML file or a PDF file on Canvas. Also, please upload it to your website.

Save the file (found on Canvas) crime_simple.txt to the same folder as this file (your Rmd file for Assignment 3).

Load the data.
```{r}
setwd("/Users/johannadoherty/Documents/FALL 21/CRIM 250/Assign 3")
library(readr)
library(knitr)
dat.crime <- read_delim("crime_simple.txt", delim = "\t")
```

This is a dataset from a textbook by Brian S. Everitt about crime in the US in 1960. The data originate from the Uniform Crime Report of the FBI and other government sources. The data for 47 states of the USA are given. 

Here is the codebook:

R: Crime rate: # of offenses reported to police per million population

Age: The number of males of age 14-24 per 1000 population

S: Indicator variable for Southern states (0 = No, 1 = Yes)

Ed: Mean of years of schooling x 10 for persons of age 25 or older

Ex0: 1960 per capita expenditure on police by state and local government

Ex1: 1959 per capita expenditure on police by state and local government

LF: Labor force participation rate per 1000 civilian urban males age 14-24

M: The number of males per 1000 females

N: State population size in hundred thousands

NW: The number of non-whites per 1000 population

U1: Unemployment rate of urban males per 1000 of age 14-24

U2: Unemployment rate of urban males per 1000 of age 35-39

W: Median value of transferable goods and assets or family income in tens of $

X: The number of families per 1000 earning below 1/2 the median income


We are interested in checking whether the reported crime rate (# of offenses reported to police per million population) and the average education (mean number of years of schooling for persons of age 25 or older) are related. 


1. How many observations are there in the dataset? To what does each observation correspond?

```{r}
dim(dat.crime)
```

__There are 14 columns and 47 observations (or rows) in this data set. Each observation corresponds to a different state in the US.__

2. Draw a scatterplot of the two variables. Calculate the correlation between the two variables. Can you come up with an explanation for this relationship?

```{r, fig.width=6, fig.height=4}
plot(dat.crime$Ed, dat.crime$R, main="Relationship between Education Level and Reported Crime",
    xlab="Education Level in Yearsx10", ylab="Reported Crime per Million Population")
cor(dat.crime$Ed, dat.crime$R)
```

__There is not an extremely defined relationship here, but it is slightly positively correlated with a correlation of .3228 as calculated above. A possible explanation for this relationship is that areas with people that have had more education may be more economically affluent and may be more likely to report crime that occurs. Since crime is more common in areas with lower socioeconomic statuses, people in these areas may be less likely to report given the commonality of these occurrences. So even if there are lower actual rates of crime in areas with higher education, there may be more reported crime.__

3. Regress reported crime rate (y) on average education (x) and call this linear model `crime.lm` and write the summary of the regression by using this code, which makes it look a little nicer `{r, eval=FALSE} kable(summary(crime.lm)$coef, digits = 2)`.

```{r} 
# Remember to remove eval=FALSE above!
crime.lm <- lm(formula = R ~ Ed, data = dat.crime)
summary(crime.lm)
```

4. Are the four assumptions of linear regression satisfied? To answer this, draw the relevant plots. (Write a maximum of one sentence per assumption.)

```{r} 
plot(crime.lm, which=1)
```

__1)Linearity: as seen in the scatterplot from question 2, the relationship between the two variables does not look straight, and also looking at the Residuals vs. Fitted plot, the data points are not evenly spread along the regression line, so there seems to be an issue of non-constant variance and the linearity assumption is not met. .__
```{r} 
plot(dat.crime$Ed, crime.lm$residuals, ylim=c(-15,15), main="Residuals vs. x", xlab="x, Education", ylab="Residuals")
abline(h = 0, lty="dashed")
```

__2) Independence: As seen in the above residuals vs. x plot, there are no patterns meaning that the data are likely independent of each other.__
```{r} 
plot(dat.crime$R, dat.crime$Ed)
```

__3) Homoscedasticity: In the y,x scatterplot above, the relationship between the data does not look linear, indicating a difference in the variability in y values of x, so this assumption is not met.__
```{r} 
plot(crime.lm, which=2)
```

__4)Normal Population: The Normal QQ plot does not look great both at the top and the bottom. It looks pretty normally distributed in the middle, but the top is pretty far off so we can not assume this condition is satisfied..__

5. Is the relationship between reported crime and average education statistically significant? Report the estimated coefficient of the slope, the standard error, and the p-value. What does it mean for the relationship to be statistically significant?

__Given that the R-squared value of the linear regression is low (0.1042), the relationship between reported crime and average education is not statistically significant. The coefficient of the slope is 1.1161, and the p-value is 0.02668. If the relationship were statistically significant it would mean that changes in the dependent variable are correlated with changes in the independent variable.__

6. How are reported crime and average education related? In other words, for every unit increase in average education, how does reported crime rate change (per million) per state?

__The slope in our model tells us that for every 1 year increase in the average length of education in a given state, the # of crimes reported to the police per million population for that state will increase by 1.1161. This is not a causal interpretation, but this is the predicted relationship between reported crime and average education.__

7. Can you conclude that if individuals were to receive more education, then crime will be reported more often? Why or why not?

__There are two issues with making this conclusion based on this data. Firstly linear regression provides a prediction based on a correlation and should not be used to make causal statements. If anything, one could say that more education is related to more reporting of crime, but no causal statements should be made. Secondly, it is difficult to assume either of these statements are accurate when some of the assumptions were not met. If all four assumptions for linear regression had been satisfied then we would have more confidence in the predictions made using this regression. Since this is not the case, it is possible that a linear model is not the right fit for this relationship, making it hard to make even correlational judgments based on this regression.  __



#title: "Exam 2"
##author: "Johanna Doherty"
date: "11/01/2021"
output: html_document


__Instructions__

a. Create a folder in your computer (a good place would be under Crim 250, Exams). 

b. Download the dataset from the Canvas website (sim.data.csv) onto that folder, and save your Exam 2.Rmd file in the same folder.

c. Data description: This dataset provides (simulated) data about 200 police departments in one year. It contains information about the funding received by the department as well as incidents of police brutality. Suppose this dataset (sim.data.csv) was collected by researchers to answer this question: **"Does having more funding in a police department lead to fewer incidents of police brutality?"**
d. Codebook:
- funds: How much funding the police department received in that year in millions of dollars.
- po.brut: How many incidents of police brutality were reported by the department that year.
- po.dept.code: Police department code

__Problem 1: EDA (10 points)__

Describe the dataset and variables. Perform exploratory data analysis for the two variables of interest: funds and po.brut.

```{r}
setwd("/Users/johannadoherty/Documents/FALL 21/CRIM 250/EXAM 2")
dat <- read.csv(file = 'sim.data.csv')
names(dat)
dim(dat)
summary(dat)

plot(dat$funds, dat$po.brut, main="Scatterplot Comparing Funds and Police Brutality", xlab="Funds, in millions", ylab="Police Brutality Incidents")
```

__This data set has 3 columns with 200 rows, which represents 200 police departments' data for the three variables: police department code, police brutality incidents, and funding available to the department. As we can see in the scatterplot above showing the relationship between the amount of funding and the quantity of police brutality incidents, there is a pretty clear negative correlation. From looking at this scatterplot it seems as though increased funding is correlated with fewer police brutality incidents. .__


__Problem 2: Linear regression (30 points)__

a. Perform a simple linear regression to answer the question of interest. To do this, name your linear model "reg.output" and write the summary of the regression by using "summary(reg.output)". 

```{r}
# Remember to remove eval=FALSE!!
reg.output <- lm(formula = dat$po.brut ~ dat$funds, data = dat)
summary(reg.output)
```


b. Report the estimated coefficient, standard error, and p-value of the slope. Is the relationship between funds and incidents statistically significant? Explain.

__The estimated coefficient is -0.367099, the standard error is 0.004496, and the p-value of the slope is 2.2e-16.  Given that the p-value is extremely small, and assuming we are using 0.05 as our significance value, then the relationship between funds and brutality incidents is statistically significant. This p-value essentially tells us that if the null hypothesis (no relationship between the two variables) were true, the chances of observing a statistic like one would be 2.2e-16, which is a very small chance. Given the summary of this linear regression, it seems that there is a pretty strong negative association between the two variables. Looking at this data and the regression we can say that a one million dollar increase in funding for a department is associated with .367099 fewer incidents of police brutality for a given year.__

c. Draw a scatterplot of po.brut (y-axis) and funds (x-axis). Right below your plot command, use abline to draw the fitted regression line, like this:
```{r}
# Remember to remove eval=FALSE!!
plot(dat$funds, dat$po.brut, main = "Regression of Funds over Brutality", xlab="Funds in millions", ylab="Brutality Incidents")
abline(reg.output, col = "red", lwd=2)
```
Does the line look like a good fit? Why or why not?

__The line looks like an ok fit, but the points definitely stray from the line at the top and bottom of the regression line. While it is certainly not a perfect linear fit, it seems close enough to continue with the analysis and check the other assumptions. __

d. Are the four assumptions of linear regression satisfied? To answer this, draw the relevant plots. (Write a maximum of one sentence per assumption.) If not, what might you try to do to improve this (if you had more time)?

```{r}
plot(dat$funds, dat$po.brut, main="Scatterplot Comparing Funds and Police Brutality", xlab="Funds, in millions", ylab="Police Brutality Incidents")
```

__1. Linearity: Using a scatterplot of x,y or funds,po.brut we can see that the relationship between the two variables looks vaguely linear. It is not perfectly linear but it is close enough to say that the assumption has been satisfied for the purposes of this analysis. __

```{r}
plot(dat$funds, reg.output$residuals, main = "Residuals vs. X Plot", xlab="X, Funds", ylab="Residuals")
abline(h = 0, lty="dashed")
```

__2. Independence: Looking at the residuals vs. X plot above, there is a definite pattern to the data, meaning that the data are likely not independent of each other and this assumption is not satisfied.__

```{r}
plot(dat$po.brut, dat$funds, main="Y vs. X Scatterplot")
```

__3.Homoscedasticity: Looking at the y vs. x scatterplot above, there is no fan shape or tendency for the data to grow or shrink in any given area observed. This means that this assumption has been met and that the variability in y about the same for all values of x. __

```{r}
plot(reg.output, which=2)
```

__4. Normality: In the Normal QQ plot seen above, we can see that the points stray pretty far from the line at the top and bottom of the line. While the section of the  data that follows the line has many more points, there are still a lot of points at the tails that do not follow the line and stray pretty far. Based on this, the normality assumption is not met for this data.  __

__In order to fix this lack of model fit, I would use data transformation, like taking the log of y or squaring y, to see if these changes make the data fit the model a little better. If the transformed data fits the model more accurately, we can then proceed with analysis and later un-transform the results to make accurate statements.__

e. Answer the question of interest based on your analysis.

__Based on the linear regression analysis above, there are a few factors to consider in terms of answering the question of interest. Based on the summary of the regression, there is a strong negative correlation with an R-squared of .9712, and the relationship is statistically significant given that the p-value is 2.2e-16, which is much smaller than our significance value of 0.05. Using the regression model, we would say that a one million dollar increase in funding for a department is associated with .367099 fewer incidents of police brutality for a given year. However, given that two of the assumptions have not been met, and that the two that have been met are not very solidly met, it's not clear that we are really able to use this regression model with confidence. It's possible that while there is a clear relationship between these variables, it may not be a linear relationship, which means that this regression model might not be the best fit for this data.__

__Problem 3: Data ethics (10 points)__

Describe the dataset. Considering our lecture on data ethics, what concerns do you have about the dataset? Once you perform your analysis to answer the question of interest using this dataset, what concerns might you have about the results?

__This data set is based on simulated data from 200 police departments. My concern with this data set lies in what information the algorithm/simulation was based on and how representative it truly is of real data from real departments. I would like more information on how the simulated data was created/collected and what testing it went through to make sure that it is truly representative of the results from a range of diverse real departments. I'm also curious about the size of departments used, if we are comparing the Philadelphia PD to a tiny department in the middle of PA, then using the raw number of incidents would not be a good comparison given the difference in sizes. There would need to be a lot more clarity about the source of the data in order to use this analysis with more confidence.__ 

__Given these concerns about the source of the data, after running my analysis my next concern would be that these results/relationship would be used to make assumptions about all police forces, regardless of whether the data is truly representative or not. Additionally, there are probably some confounding variables in this relationship, and it would be important to look at what is really causing this relationship before definitively deciding that increasing funding will reduce police brutality. We would also have to consider the effect that increasing funding to the police might have in other areas besides brutality. It seems to me that if the funding is not being spent in the right places, it's possible it wouldn't have the same effect as this data shows and could possibly even have a negative effect. Overall, given that the data is simulated, if is based on flawed or non-representative data, then any assumptions we make based on the data will not be accurate and should not be used to inform policy.__


---
  title: "Assignment 4"
output: html_document
---
  
  __Chapter 3: Data Visualizations:__
```{r}
#eval=FALSE
library(tidyverse)
#> ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──
#> ✔ ggplot2 3.3.2     ✔ purrr   0.3.4
#> ✔ tibble  3.0.3     ✔ dplyr   1.0.2
#> ✔ tidyr   1.1.2     ✔ stringr 1.4.0
#> ✔ readr   1.4.0     ✔ forcats 0.5.0
#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
#> ✖ dplyr::filter() masks stats::filter()
#> ✖ dplyr::lag()    masks stats::lag()
```
__The above code is loading the tidyverse library which will allow us to have access to data sets, help, and functions that might be needed when using ggplot. The first line loads the tidyverse, and the rest of the information tells us which parts of base R conflict with the tidyverse library.__

```{r, eval=FALSE}

mpg
#> # A tibble: 234 x 11
#>   manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class 
#>   <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> 
#> 1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…
#> 2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…
#> 3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…
#> 4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…
#> 5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…
#> 6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…
#> # … with 228 more rows
```

__This mpg data frame contains observations collected by the US Environmental Protection Agency on 38 models of car, and contains variables such as a car's engine size and a car's fuel efficiency on the highway. We can use this data frame in our analysis using ggplot tools. __

```{r, eval=FALSE}

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))

```
__This code tells R to make a ggplot using the mpg data frame, and places displ on the x axis and hwy on the y axis. __


```{r, eval=FALSE}
ggplot(data = DATA) + GEOM_FUNCTION(mapping = aes(MAPPINGS))

```
__This code creates a template to make plots in gg plot so that we can just replace the bracketed sections with what we want to plot instead of writing the code out entirely each time.__



```{r, eval=FALSE}
# eval = FALSE
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = class))

```
__This line of code maps aesthetics onto the variables in the plot. This particular line is mapping colors of data points to the class variable so that the class of each car can be easily seen in the scatterplot.__




```{r, eval=FALSE}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, size = class))

#> Warning: Using size for a discrete variable is not advised.
```
__This line of code continues with mapping aesthetics but instead of color it is mapping size onto the class variable. in this case, the class of each car can be seen by the size of the point it represents on the scatterplot.__


```{r, eval=FALSE}
# Left
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, alpha = class))


# Right
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, shape = class))

```
__These two chunks of code map transparency and shape onto the class variable, respectively. The class of each car is shown by the transparency on the first plot and by the shape on the second plot.__


```{r, eval=FALSE}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), color = "blue")

```
__This code changes the color of all the points on the plot to blue, without having the color attached to any meaning or variable.__


```{r, eval=FALSE}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)

```
__This code allows you to split the data into separate subplots, or facets. The variable that is being split into facets in this code is class.__


```{r, eval=FALSE}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ cyl)

```
__This code allows you to facet the plot based on the combination of two variables. We can use facet_grid, and then input the two variables in the parentheses.__


```{r, eval=FALSE}
# left
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))


# right
ggplot(data = mpg) + 
  geom_smooth(mapping = aes(x = displ, y = hwy))
```
__These two lines of code make plots using two different geoms for the same data. The top chunk uses points and the bottom chunk uses a smooth geom to represent the data.__


```{r, eval=FALSE}
ggplot(data = mpg) + 
  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))

```
__This code uses the geom smooth to separate the data into 3 different lines based on their value for variable drv. This allows us to see how data points with each response for drv look when mapped separately.__

```{r, eval=FALSE}
ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy))

ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy, group = drv))


ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy, color = drv),show.legend = FALSE)
```
__This code lets us set the group aesthetic to a categorical variable to draw multiple objects, and ggplot2 will draw a separate object for each unique value of the grouping variable.__

```{r, eval=FALSE}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) +
  geom_smooth(mapping = aes(x = displ, y = hwy))

```
__This code allows us to display multiple geoms in the same plot, by adding multiple geom functions to ggplot.__

```{r, eval=FALSE}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point() + 
  geom_smooth()

```
__This code avoids repetition by passing a set of mappings to ggplot. ggplot2 will apply these mappings to each geom in the graph. This code will produce the same plot as the previous code, while avoiding any potential repetitions.__

```{r, eval=FALSE}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(mapping = aes(color = class)) + 
  geom_smooth()

```
__This code allows us to place mappings in a geom function, which ggplot2 will then treat as local mappings for the intended layer. ggplot2 will then use these local mappings to overwrite the global mappings for that layer only. Because of this we can display different aesthetics in different layers.__

```{r, eval=FALSE}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(mapping = aes(color = class)) + 
  geom_smooth(data = filter(mpg, class == "subcompact"), se = FALSE)

```
__This code allows us to do something similar to above, but instead of changing aesthetics for each layer we can change the specified data for each layer.__

```{r, eval=FALSE}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut))

```
__This code creates a bar chart for the diamonds data set using cut and count. Even though count is not a variable in diamonds, ggplot can bin your data and then plot bin counts, the number of points that fall in each bin.__


```{r, eval=FALSE}
ggplot(data = diamonds) + 
  stat_count(mapping = aes(x = cut))

```
__This code will give us the same plot as above, but using stat_count instead of geom_bar to create the count data points.__


```{r, eval=FALSE}
demo <- tribble(
  ~cut,         ~freq,
  "Fair",       1610,
  "Good",       4906,
  "Very Good",  12082,
  "Premium",    13791,
  "Ideal",      21551
)

ggplot(data = demo) +
  geom_bar(mapping = aes(x = cut, y = freq), stat = "identity")

```
__In the code above, we can change the stat of geom_bar from count to identity. This lets us map the height of the bars to the raw values of a y variable.__

```{r, eval=FALSE}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = stat(prop), group = 1))

```
__We can also override the default mapping from transformed variables to aesthetics. In this code we can create a bar chart of proportion instead of count.__


```{r, eval=FALSE}
ggplot(data = diamonds) + 
  stat_summary(mapping = aes(x = cut, y = depth),
    fun.min = min,
    fun.max = max,
    fun = median)

```
__This code uses stat_summary to draw more attention to the transformations in the data. It summarizes the y values for each x value, to draw attention to the summary that we are computing.__


```{r, eval=FALSE}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, colour = cut))
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = cut))

```

__Both the color aesthetic and fill allow us to color the bar chart for a more visual display of the data.__

```{r, eval=FALSE}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity))

```
__This code maps fill onto another variable, which creates an automatically color-coded stacked bar chart with the data.__


```{r, eval=FALSE}
ggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + 
  geom_bar(alpha = 1/5, position = "identity")
ggplot(data = diamonds, mapping = aes(x = cut, colour = clarity)) + 
  geom_bar(fill = NA, position = "identity")

```
__This code is used to place each object exactly where it falls in the context of the graph. When used on bar graphs, it can overlap the bars, so to see the overlapping we can make the bars slightly transparent by setting alpha to a small value, or completely transparent by setting fill = NA.__

```{r, eval=FALSE}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "fill")

```
__This code makes each set of stacked bars the same height, so that we can more easily compare proportions across groups.__

```{r, eval=FALSE}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "dodge")

```
__This code places overlapping objects right next to one another so that we can more easily compare individual values.__

```{r, eval=FALSE}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), position = "jitter")

```
__We can avoid the isseu of gridding and overplotting by setting the position adjustment to “jitter" which will add a small amount of random noise to each point. This code will spread the points out more because no two points are likely to receive the same amount of random noise, and make it easier to see what is really going on with the data.__

```{r, eval=FALSE}
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot()
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot() +
  coord_flip()

```
__This code switches the x and y axes on the graph, which can be used to make horizontal boxplots or to avoid too long labels.__


```{r, eval=FALSE}
nz <- map_data("nz")

ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black")

ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_quickmap()

```
__This code sets the aspect ratio correctly for maps, which is important to use when plotting spatial data with ggplot2__


```{r, eval=FALSE}
bar <- ggplot(data = diamonds) + 
  geom_bar(
    mapping = aes(x = cut, fill = cut), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

bar + coord_flip()
bar + coord_polar()

```
__This code uses polar coordinates, which can reveal interesting connections between a bar chart and a Coxcomb chart.__

```{r, eval=FALSE}
ggplot(data = DATA) + GEOM_FUNCTION>(mapping = aes(MAPPINGS) stat = STAT> position = POSITION) + COORDINATE_FUNCTION +  FACET_FUNCTION

```
__The above code is our new template, with added position adjustments, stats, coordinate systems, and faceting.__

__Chapter 28: Graphics for Communication__

```{r, eval=FALSE}
library(tidyverse)
```
__The above code is once again loading the tidyverse library which will allow us to have access to data sets, help, and functions that might be needed when using ggplot__

```{r, eval=FALSE}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  labs(title = "Fuel efficiency generally decreases with engine size")

```
__The above code adds labels to the graph with the labs() function, this example adds a plot title. This is useful when turning an exploratory graphic into an expository graphic.__


```{r, eval=FALSE}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  labs( title = "Fuel efficiency generally decreases with engine size",
    subtitle = "Two seaters (sports cars) are an exception because of their light weight",
    caption = "Data from fueleconomy.gov" )

```
__This code adds subtitles, which adds additional detail in a smaller font beneath the title, and a caption, which adds text at the bottom right of the plot, often used to describe the source of the data.__

```{r, eval=FALSE}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_smooth(se = FALSE) +
  labs(
    x = "Engine displacement (L)",
    y = "Highway fuel economy (mpg)",
    colour = "Car type"
  )

```
__This code replaces the axis and legend titles with a more in-depth description including unit for more clarity.__

```{r, eval=FALSE}
df <- tibble(
  x = runif(10),
  y = runif(10)
)
ggplot(df, aes(x, y)) +
  geom_point() +
  labs(
    x = quote(sum(x[i] ^ 2, i == 1, n)),
    y = quote(alpha + beta + frac(delta, theta))
  )

```
__This code allows us to use mathematical equations rather than text strings as labels for the graphs.__


```{r, eval=FALSE}
best_in_class <- mpg %>%
  group_by(class) %>%
  filter(row_number(desc(hwy)) == 1)

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_text(aes(label = model), data = best_in_class)

```
__We can use this code to label individual observations or groups of observations, to make it easier to point out the interesting parts of a graph. It's good to use geom_text() because it has an additional aesthetic: label, which lets us add textual labels to plots.__


```{r, eval=FALSE}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class))

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  scale_x_continuous() +
  scale_y_continuous() +
  scale_colour_discrete()

```
__These two chunks show that when you type the first chunk of code, ggplot will automatically add default scales behind the scenes, which are shown in the second chunk of code.__

```{r, eval=FALSE}
ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  scale_y_continuous(breaks = seq(15, 40, by = 5))

```
__This code uses breaks by overriding the default choice.Breaks controls the position of the ticks, or the values associated with the keys.__

```{r, eval=FALSE}
ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  scale_x_continuous(labels = NULL) +
  scale_y_continuous(labels = NULL)

```
__This code uses labels, but sets them to null to suppress the labels altogether, which is useful for maps, or for publishing plots where you can’t share the absolute numbers.__

```{r, eval=FALSE}
presidential %>%
  mutate(id = 33 + row_number()) %>%
  ggplot(aes(start, id)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    scale_x_date(NULL, breaks = presidential$start, date_labels = "'%y")

```
__This code is useful for when you have relatively few data points and allows us to highlight exactly where the observations occur.__


```{r, eval=FALSE}
base <- ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class))

base + theme(legend.position = "left")
base + theme(legend.position = "top")
base + theme(legend.position = "bottom")
base + theme(legend.position = "right") # the default

```
__we can use theme settings to control the overall position of the legend. The theme setting legend.position controls where the legend is drawn.__

```{r, eval=FALSE}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_smooth(se = FALSE) +
  theme(legend.position = "bottom") +
  guides(colour = guide_legend(nrow = 1, override.aes = list(size = 4)))
#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'

```
__This code controls the number of rows the legend uses with nrow, and overrides one of the aesthetics to make the points bigger.__

```{r, eval=FALSE}
ggplot(diamonds, aes(carat, price)) +
  geom_bin2d()

ggplot(diamonds, aes(log10(carat), log10(price))) +
  geom_bin2d()

```
__This code log transforms the data so that we can see the precise relationships more clearly.__

```{r, eval=FALSE}
ggplot(diamonds, aes(carat, price)) +
  geom_bin2d() + 
  scale_x_log10() + 
  scale_y_log10()

```
__After the transformation, the axes are now labelled with transformed values, so we can instead do it with the scale. This is visually identical, except the axes are labelled on the original data scale.__

```{r, eval=FALSE}
df <- tibble(
  x = rnorm(10000),
  y = rnorm(10000)
)
ggplot(df, aes(x, y)) +
  geom_hex() +
  coord_fixed()

ggplot(df, aes(x, y)) +
  geom_hex() +
  viridis::scale_fill_viridis() +
  coord_fixed()

```
__This code lets us use scale_colour_viridis which is provided by the viridis package. It’s a continuous analog of the categorical ColorBrewer scales. it is a  continuous colour scheme that has good perceptual properties.__

```{r, eval=FALSE}
ggplot(mpg, mapping = aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth() +
  coord_cartesian(xlim = c(5, 7), ylim = c(10, 30))

mpg %>%
  filter(displ >= 5, displ <= 7, hwy >= 10, hwy <= 30) %>%
  ggplot(aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth()

```
__This code is an example of how to control the plot limits by either Adjusting what data are plotted, Setting the limits in each scale, or Setting xlim and ylim in coord_cartesian.__

```{r, eval=FALSE}
suv <- mpg %>% filter(class == "suv")
compact <- mpg %>% filter(class == "compact")
eval=FALSE

ggplot(suv, aes(displ, hwy, colour = drv)) +
  geom_point()
eval=FALSE

ggplot(compact, aes(displ, hwy, colour = drv)) +
  geom_point()

```
__We can also set the limits on individual scales. When we reduce the limits, it is basically equivalent to subsetting the data. It is generally more useful if you want expand the limits, for example, to match scales across different plots.__ 


```{r, eval=FALSE}
x_scale <- scale_x_continuous(limits = range(mpg$displ))
y_scale <- scale_y_continuous(limits = range(mpg$hwy))
col_scale <- scale_colour_discrete(limits = unique(mpg$drv))


ggplot(suv, aes(displ, hwy, colour = drv)) +
  geom_point() +
  x_scale +
  y_scale +
  col_scale


ggplot(compact, aes(displ, hwy, colour = drv)) +
  geom_point() +
  x_scale +
  y_scale +
  col_scale

```
__This code attempts to overcome the plot limits problem by sharing scales across multiple plots, training the scales with the limits of the full data.__

```{r, eval=FALSE}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  theme_bw()

```
__we can customise the non-data elements of your plot with a theme as seen in the above code.__

```{r, eval=FALSE}
ggsave("my-plot.pdf")
#> Saving 7 x 4.33 in image

```
__ggsave helps to get the plots out of R and into the final write up by saving them to the disk.__




